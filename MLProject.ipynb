{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiH5omaD6ihV",
    "outputId": "5c70325f-4881-407a-c445-abd82e649b00"
   },
   "outputs": [],
   "source": [
    "!pip install spacy_transformers\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Create functions to parse and import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T19:58:49.138716Z",
     "start_time": "2025-03-02T19:58:49.124440Z"
    }
   },
   "source": [
    "import spacy, json\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def json_parser(data):\n",
    "  parsed_data = []\n",
    "  for item in data:\n",
    "    text = item[\"text\"]\n",
    "    labels = []\n",
    "    for entity in item[\"label\"]:\n",
    "      label = entity[\"labels\"][0].capitalize()\n",
    "      start = entity[\"start\"]\n",
    "      end = entity[\"end\"]\n",
    "      labels.append((start, end, label))\n",
    "    parsed_data.append((text, labels))\n",
    "  return parsed_data\n",
    "\n",
    "def pubtator_extractor(data):\n",
    "  \"\"\"Converts data from pubtator to Spacy's JSON Format\"\"\"\n",
    "  parsed_data = []\n",
    "  for line in data:\n",
    "    line = line.strip()\n",
    "    if line == \"\":\n",
    "       if parsed_entity:\n",
    "        parsed_data.append(tuple(parsed_entity))\n",
    "    if \"|t|\" in line:\n",
    "      parsed_entity = []\n",
    "      current_title = line.split(\"|t|\")[1] + \" \"\n",
    "    elif \"|a|\" in line:\n",
    "      parsed_entity.append(current_title + line.split(\"|a|\")[1])\n",
    "    elif \"Disease\" in line or \"Chemical\" in line:\n",
    "      if len(parsed_entity) == 1:\n",
    "        parsed_entity.append([])\n",
    "      start = int(line.split(\"\\t\")[1])\n",
    "      end = int(line.split(\"\\t\")[2])\n",
    "      label = line.split(\"\\t\")[4]\n",
    "      parsed_entity[1].append((start, end, label))\n",
    "  return parsed_data\n",
    "\n",
    "def db_creator(data):\n",
    "  db = DocBin()\n",
    "  nlp = spacy.blank(\"en\")\n",
    "\n",
    "  for text, annotations in data:\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "      span = doc.char_span(start, end, label=label)\n",
    "      if span is None:\n",
    "        continue\n",
    "      else:\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "  return db\n",
    "\n",
    "def db_creator_spans(data):\n",
    "  db = DocBin()\n",
    "  nlp = spacy.blank(\"en\")\n",
    "\n",
    "  for text, annotations in data:\n",
    "    doc = nlp(text)\n",
    "    spans = []\n",
    "    for start, end, label in annotations:\n",
    "      span = doc.char_span(start, end, label=label)\n",
    "      if span is None:\n",
    "        continue\n",
    "      else:\n",
    "        spans.append(span)\n",
    "    doc.spans[\"sc\"] = spans\n",
    "    db.add(doc)\n",
    "  return db\n",
    "\n",
    "def sorted_scores(nlp , data):\n",
    "  whole_text = []\n",
    "  for text_tuple in data:\n",
    "      text = text_tuple[0]\n",
    "      doc = nlp(text)\n",
    "      single_text = []\n",
    "      if \"sc\" in doc.spans:\n",
    "          for i, span in enumerate(doc.spans[\"sc\"]):\n",
    "              score = doc.spans[\"sc\"].attrs[\"scores\"][i]\n",
    "              span_text = span.text\n",
    "              single_text.append((span_text, score))\n",
    "      else:\n",
    "          print(f\"No spans found in: '{text}'\")\n",
    "      if len(single_text) > 0:\n",
    "        whole_text.append([text , sorted(single_text, key=lambda x: x[1])])\n",
    "  text_list = sorted(whole_text, key=lambda x: x[1][0][1])\n",
    "  text_percent = []\n",
    "  for data in text_list:\n",
    "    text = data[0]\n",
    "    percent = sum(x[1] for x in data[1]) / len(data[1]) * 100\n",
    "    text_percent.append([text, percent])\n",
    "  return sorted(text_percent, key=lambda x: x[1])\n",
    "\n",
    "def get_sorted_false_negatives(nlp, examples):\n",
    "    \"\"\"\n",
    "    Identifies and sorts false negatives in a spaCy spancat model for active learning.\n",
    "\n",
    "    Args:\n",
    "        nlp (spacy.Language): The trained spaCy model.\n",
    "        examples (list): List of (text, gold_spans) tuples, where\n",
    "                         gold_spans is a list of (start, end, label).\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of (text, missing_spans) tuples prioritized for active learning.\n",
    "    \"\"\"\n",
    "    false_negatives = []\n",
    "\n",
    "    for text, gold_spans in examples:\n",
    "        doc = nlp(text)\n",
    "        predicted_spans = set((span.start_char, span.end_char, span.label_) for span in doc.spans.get(\"sc\", []))\n",
    "        gold_spans_set = set(gold_spans)\n",
    "\n",
    "        missing_spans = gold_spans_set - predicted_spans  # False negatives\n",
    "\n",
    "        if missing_spans:\n",
    "            # Store example with count and average span length\n",
    "            avg_span_length = sum(e - s for s, e, _ in missing_spans) / len(missing_spans)\n",
    "            false_negatives.append((text, list(missing_spans), len(missing_spans), avg_span_length))\n",
    "\n",
    "    # Sort first by number of false negatives, then by average span length\n",
    "    false_negatives.sort(key=lambda x: (-x[2], -x[3]))\n",
    "\n",
    "    # Return sorted results without the sorting metadata\n",
    "    return [(text, missing_spans) for text, missing_spans, _, _ in false_negatives]"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rcmiY9Ka80y0",
    "ExecuteTime": {
     "end_time": "2025-03-02T19:58:54.136370Z",
     "start_time": "2025-03-02T19:58:54.029946Z"
    }
   },
   "source": [
    "with open(\"data/pubtator_files/CDR_DevelopmentSet.PubTator.txt\", mode=\"r\") as f:\n",
    "  dev_data = pubtator_extractor(f.readlines())\n",
    "  f.close()\n",
    "\n",
    "with open(\"data/pubtator_files/CDR_TrainingSet.PubTator.txt\", mode=\"r\") as f:\n",
    "  train_data_full = pubtator_extractor(f.readlines())\n",
    "  f.close()\n",
    "\n",
    "with open(\"data/training_data/initial_annotated_train.json\", mode=\"r\") as f:\n",
    "  train_data = json_parser(json.load(f))\n",
    "  f.close()"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:23:17.153841Z",
     "start_time": "2025-03-02T14:23:13.167802Z"
    }
   },
   "outputs": [],
   "source": [
    "train = db_creator_spans(train_data)\n",
    "train.to_disk(\"train_spans.spacy\")\n",
    "\n",
    "dev = db_creator_spans(dev_data)\n",
    "dev.to_disk(\"dev_spans.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:38:53.386192Z",
     "start_time": "2025-03-02T12:07:41.538793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./spacy/config_span.cfg --output ./models/initial_model --paths.train ./data/spacy_db/train_spans.spacy --paths.dev ./data/spacy_db/dev_spans.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:24:58.635138Z",
     "start_time": "2025-03-02T14:23:17.170132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4m[i] Using CPU\u001B[0m\n",
      "\u001B[38;5;4m[i] Per-component scores will be saved to output JSON file.\u001B[0m\n",
      "\u001B[38;5;2m[+] Saved results to results\\result_initial_model.json\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark accuracy .\\models\\initial_model\\model-best\\ .\\data\\spacy_db\\dev_spans.spacy -o results/result_initial_model.json -P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_text = [text[0] for text in train_data_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take 25 texts from the data with the least overall confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"models/initial_model/model-best\")\n",
    "datas = sorted_scores(nlp, train_data_full)\n",
    "with open(\"data/training_data/active_learning_1_test.txt\", mode=\"w\") as f:\n",
    "    for line in datas[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:26:25.844297Z",
     "start_time": "2025-03-02T14:26:25.834151Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/training_data/active_learning_1.json\", mode=\"r\") as f:\n",
    "    active_learning_data_1 = json_parser(json.load(f))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:26:28.040199Z",
     "start_time": "2025-03-02T14:26:27.484374Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_active_1 = train_data + active_learning_data_1\n",
    "training_data_active_1 = db_creator_spans(training_data_active_1)\n",
    "training_data_active_1.to_disk(\"data/spacy_db/training_data_active_1.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python -m spacy train ./spacy/config_span.cfg --output ./models/active_1 --paths.train ./data/spacy_db/training_data_active_1.spacy --paths.dev ./data/spacy_db/dev_spans.spacy"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:58:55.930067Z",
     "start_time": "2025-03-02T14:57:16.116356Z"
    }
   },
   "source": "!python -m spacy benchmark accuracy .\\models\\active_1\\model-best\\ .\\data\\spacy_db\\dev_spans.spacy -o results/result_active_1.json -P",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4m[i] Using CPU\u001B[0m\n",
      "\u001B[38;5;4m[i] Per-component scores will be saved to output JSON file.\u001B[0m\n",
      "\u001B[38;5;2m[+] Saved results to result_active_1.json\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try getting the scores after the first iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"models/output_spans_1/model-best\")\n",
    "datas = sorted_scores(nlp, train_data_full)\n",
    "datas[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/training_data/active_learning_2_test.txt\", mode=\"w\") as f:\n",
    "    for line in datas[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:01:43.916031Z",
     "start_time": "2025-03-02T15:01:42.967530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/training_data/active_learning_2.json\", mode=\"r\") as f:\n",
    "    active_learning_data_2 = json_parser(json.load(f))\n",
    "    f.close()\n",
    "training_data_active_2 = train_data + active_learning_data_1 + active_learning_data_2\n",
    "training_data_active_2 = db_creator_spans(training_data_active_2)\n",
    "training_data_active_2.to_disk(\"data/spacy_db/training_data_active_2.spacy\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python -m spacy train ./spacy/config_span.cfg --output ./models/active_2 --paths.train ./data/spacy_db/training_data_active_2.spacy --paths.dev ./data/spacy_db/dev_spans.spacy"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!python -m spacy benchmark accuracy .\\models\\active_2\\model-best\\ .\\data\\spacy_db\\dev_spans.spacy -o results/result_active_2.json -P"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try to improve the recall score for this is cycle by targeting on reducing the false negatives"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:49:26.740189Z",
     "start_time": "2025-03-02T15:47:49.494885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"models/active_2/model-best\")\n",
    "data = get_sorted_false_negatives(nlp, train_data_full)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:49:34.521622Z",
     "start_time": "2025-03-02T15:49:34.504309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/training_data/active_learning_3.txt\", mode=\"w\") as f:\n",
    "    for line in data[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:55:42.466701Z",
     "start_time": "2025-03-02T18:55:41.738335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/training_data/active_learning_3.json\", mode=\"r\") as f:\n",
    "    active_learning_data_3 = json_parser(json.load(f))\n",
    "    f.close()\n",
    "training_data_active_3 = train_data + active_learning_data_1 + active_learning_data_2 + active_learning_data_3\n",
    "training_data_active_3 = db_creator_spans(training_data_active_3)\n",
    "training_data_active_3.to_disk(\"data/spacy_db/training_data_active_3.spacy\")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:59:59.027273Z",
     "start_time": "2025-03-02T18:59:58.427404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "active_3_ent = train_data + active_learning_data_1 + active_learning_data_2 + active_learning_data_3\n",
    "active_3_ent = db_creator(active_3_ent)\n",
    "active_3_ent.to_disk(\"data/spacy_db/active_3_ent.spacy\")"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:02:07.541426Z",
     "start_time": "2025-03-02T20:02:05.860368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(probabilities):\n",
    "    \"\"\"Compute entropy of probability distribution.\"\"\"\n",
    "    probabilities = np.array(probabilities)\n",
    "    probabilities = probabilities[probabilities > 0]  # Avoid log(0) errors\n",
    "    return -np.sum(probabilities * np.log(probabilities))\n",
    "\n",
    "def get_active_learning_samples_ner(nlp, examples, training_data, uncertainty_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Identifies false negatives & uncertain entities for active learning.\n",
    "\n",
    "    Args:\n",
    "        nlp (spacy.Language): Trained custom NER model.\n",
    "        examples (list): List of (text, gold_entities) tuples.\n",
    "        training_data (set): Already trained examples to avoid redundancy.\n",
    "        uncertainty_threshold (float): Minimum entropy score to consider an entity uncertain.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted (text, missing_entities + uncertain_entities) tuples.\n",
    "    \"\"\"\n",
    "    false_negatives = []\n",
    "    uncertain_entities = []\n",
    "\n",
    "    for text, gold_entities in examples:\n",
    "        doc = nlp(text)\n",
    "        predicted_entities = set((ent.start_char, ent.end_char, ent.label_) for ent in doc.ents)\n",
    "        gold_entities_set = set(gold_entities)\n",
    "\n",
    "        # **False Negatives**: Gold entities missed by the model\n",
    "        missing_entities = gold_entities_set - predicted_entities\n",
    "        missing_entities = [ent for ent in missing_entities if (text, ent) not in training_data]\n",
    "\n",
    "        # **Uncertain Predictions**: Entities with low confidence\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in doc.cats:\n",
    "                entity_probs = doc.cats[ent.label_]  # Get class probabilities\n",
    "                if entity_probs:\n",
    "                    ent_entropy = entropy(entity_probs)\n",
    "                    if ent_entropy >= uncertainty_threshold:  # High entropy = uncertain\n",
    "                        uncertain_entities.append((text, (ent.start_char, ent.end_char, ent.label_)))\n",
    "\n",
    "        if missing_entities:\n",
    "            avg_entity_length = sum(e - s for s, e, _ in missing_entities) / len(missing_entities)\n",
    "            false_negatives.append((text, missing_entities, len(missing_entities), avg_entity_length))\n",
    "\n",
    "    # **Sorting Criteria**:\n",
    "    # - First by number of false negatives (most gaps)\n",
    "    # - Then by avg entity length (longer entities are harder)\n",
    "    false_negatives.sort(key=lambda x: (-x[2], -x[3]))\n",
    "\n",
    "    return [(text, missing_entities + uncertain_entities) for text, missing_entities, _, _ in false_negatives]\n",
    "\n",
    "# Example usage\n",
    "nlp = spacy.load(\"models/active_3_acc/model-best\")\n"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:03:44.569840Z",
     "start_time": "2025-03-02T20:02:28.591692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get best examples for active learning\n",
    "active_samples = get_active_learning_samples_ner(nlp, train_data_full, train_data)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:03:48.903638Z",
     "start_time": "2025-03-02T20:03:48.898373Z"
    }
   },
   "cell_type": "code",
   "source": "len(active_samples)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:04:39.803617Z",
     "start_time": "2025-03-02T20:04:39.798420Z"
    }
   },
   "cell_type": "code",
   "source": "active_samples[100]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cardioprotective effect of salvianolic acid A on isoproterenol-induced myocardial infarction in rats. The present study was designed to evaluate the cardioprotective potential of salvianolic acid A on isoproterenol-induced myocardial infarction in rats. Hemodynamic parameters and lead II electrocardiograph were monitored and recorded continuously. Cardiac marker enzymes and antioxidative parameters in serum and heart tissues were measured. Assay for mitochondrial respiratory function and histopathological examination of heart tissues were performed. Isoproterenol-treated rats showed significant increases in the levels of lactate dehydrogenase, aspartate transaminase, creatine kinase and malondialdehyde and significant decreases in the activities of superoxide dismutase, catalase and glutathione peroxidase in serum and heart. These rats also showed declines in left ventricular systolic pressure, maximum and minimum rate of developed left ventricular pressure, and elevation of left ventricular end-diastolic pressure and ST-segment. In addition, mitochondrial respiratory dysfunction characterized by decreased respiratory control ratio and ADP/O was observed in isoproterenol-treated rats. Administration of salvianolic acid A for a period of 8 days significantly attenuated isoproterenol-induced cardiac dysfunction and myocardial injury and improved mitochondrial respiratory function. The protective role of salvianolic acid A against isoproterenol-induced myocardial damage was further confirmed by histopathological examination. The results of our study suggest that salvianolic acid A possessing antioxidant activity has a significant protective effect against isoproterenol-induced myocardial infarction.',\n",
       " [(759, 769, 'Chemical'),\n",
       "  (179, 197, 'Chemical'),\n",
       "  (27, 45, 'Chemical'),\n",
       "  (1222, 1240, 'Chemical'),\n",
       "  (1425, 1443, 'Chemical'),\n",
       "  (1154, 1157, 'Chemical'),\n",
       "  (1586, 1604, 'Chemical')])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:07:31.958588Z",
     "start_time": "2025-03-02T20:07:31.813801Z"
    }
   },
   "cell_type": "code",
   "source": "doc = nlp(train_data[0][0])",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:07:32.563610Z",
     "start_time": "2025-03-02T20:07:32.558951Z"
    }
   },
   "cell_type": "code",
   "source": "ent = doc.ents[0]\n",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:07:33.137534Z",
     "start_time": "2025-03-02T20:07:33.131537Z"
    }
   },
   "cell_type": "code",
   "source": "doc.cats",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
