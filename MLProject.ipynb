{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDkIbaCNklXy",
    "outputId": "0592796c-206b-4cac-f4f9-50c0666c103d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data'...\n",
      "remote: Enumerating objects: 16, done.\u001b[K\n",
      "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 16 (delta 2), reused 12 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (16/16), 1.01 MiB | 3.83 MiB/s, done.\n",
      "Resolving deltas: 100% (2/2), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sri-sandeep108/Spacy_NER_data data\n",
    "!mkdir data/results\n",
    "!mkdir data/data/spacy_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZUO5q6lklXy"
   },
   "source": [
    "Let's Create functions to parse and import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:43:51.693575Z",
     "start_time": "2025-03-04T09:43:51.677662Z"
    },
    "id": "JVJVgJZOklXz"
   },
   "outputs": [],
   "source": [
    "import spacy, json\n",
    "from spacy.tokens import DocBin\n",
    "def json_parser(data):\n",
    "  parsed_data = []\n",
    "  for item in data:\n",
    "    text = item[\"text\"]\n",
    "    labels = []\n",
    "    for entity in item[\"label\"]:\n",
    "      label = entity[\"labels\"][0].capitalize()\n",
    "      start = entity[\"start\"]\n",
    "      end = entity[\"end\"]\n",
    "      labels.append((start, end, label))\n",
    "    parsed_data.append((text, labels))\n",
    "  return parsed_data\n",
    "\n",
    "def pubtator_extractor(data):\n",
    "  \"\"\"Converts data from pubtator to Spacy's JSON Format\"\"\"\n",
    "  parsed_data = []\n",
    "  for line in data:\n",
    "    line = line.strip()\n",
    "    if line == \"\":\n",
    "       if parsed_entity:\n",
    "        parsed_data.append(tuple(parsed_entity))\n",
    "    if \"|t|\" in line:\n",
    "      parsed_entity = []\n",
    "      current_title = line.split(\"|t|\")[1] + \" \"\n",
    "    elif \"|a|\" in line:\n",
    "      parsed_entity.append(current_title + line.split(\"|a|\")[1])\n",
    "    elif \"Disease\" in line or \"Chemical\" in line:\n",
    "      if len(parsed_entity) == 1:\n",
    "        parsed_entity.append([])\n",
    "      start = int(line.split(\"\\t\")[1])\n",
    "      end = int(line.split(\"\\t\")[2])\n",
    "      label = line.split(\"\\t\")[4]\n",
    "      parsed_entity[1].append((start, end, label))\n",
    "  return parsed_data\n",
    "\n",
    "def db_creator(data):\n",
    "  db = DocBin()\n",
    "  nlp = spacy.blank(\"en\")\n",
    "\n",
    "  for text, annotations in data:\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "      span = doc.char_span(start, end, label=label)\n",
    "      if span is None:\n",
    "        continue\n",
    "      else:\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "  return db\n",
    "\n",
    "def db_creator_spans(data):\n",
    "  db = DocBin()\n",
    "  nlp = spacy.blank(\"en\")\n",
    "\n",
    "  for text, annotations in data:\n",
    "    doc = nlp(text)\n",
    "    spans = []\n",
    "    for start, end, label in annotations:\n",
    "      span = doc.char_span(start, end, label=label)\n",
    "      if span is None:\n",
    "        continue\n",
    "      else:\n",
    "        spans.append(span)\n",
    "    doc.spans[\"sc\"] = spans\n",
    "    db.add(doc)\n",
    "  return db\n",
    "\n",
    "def sorted_scores(nlp , data):\n",
    "  whole_text = []\n",
    "  for text_tuple in data:\n",
    "      text = text_tuple[0]\n",
    "      doc = nlp(text)\n",
    "      single_text = []\n",
    "      if \"sc\" in doc.spans:\n",
    "          for i, span in enumerate(doc.spans[\"sc\"]):\n",
    "              score = doc.spans[\"sc\"].attrs[\"scores\"][i]\n",
    "              span_text = span.text\n",
    "              single_text.append((span_text, score))\n",
    "      else:\n",
    "          print(f\"No spans found in: '{text}'\")\n",
    "      if len(single_text) > 0:\n",
    "        whole_text.append([text , sorted(single_text, key=lambda x: x[1])])\n",
    "  text_list = sorted(whole_text, key=lambda x: x[1][0][1])\n",
    "  text_percent = []\n",
    "  for data in text_list:\n",
    "    text = data[0]\n",
    "    percent = sum(x[1] for x in data[1]) / len(data[1]) * 100\n",
    "    text_percent.append([text, percent])\n",
    "  return sorted(text_percent, key=lambda x: x[1])\n",
    "\n",
    "def get_sorted_false_negatives(nlp, examples):\n",
    "    false_negatives = []\n",
    "\n",
    "    for text, gold_spans in examples:\n",
    "        doc = nlp(text)\n",
    "        predicted_spans = set((span.start_char, span.end_char, span.label_) for span in doc.spans.get(\"sc\", []))\n",
    "        gold_spans_set = set(gold_spans)\n",
    "\n",
    "        missing_spans = gold_spans_set - predicted_spans  # False negatives\n",
    "\n",
    "        if missing_spans:\n",
    "            # Store example with count and average span length\n",
    "            avg_span_length = sum(e - s for s, e, _ in missing_spans) / len(missing_spans)\n",
    "            false_negatives.append((text, list(missing_spans), len(missing_spans), avg_span_length))\n",
    "\n",
    "    # Sort first by number of false negatives, then by average span length\n",
    "    false_negatives.sort(key=lambda x: (-x[2], -x[3]))\n",
    "\n",
    "    # Return sorted results without the sorting metadata\n",
    "    return [(text, missing_spans) for text, missing_spans, _, _ in false_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:22:55.566579Z",
     "start_time": "2025-03-04T09:22:55.441549Z"
    },
    "id": "rcmiY9Ka80y0"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/pubtator_files/CDR_DevelopmentSet.PubTator.txt\", mode=\"r\") as f:\n",
    "  dev_data = pubtator_extractor(f.readlines())\n",
    "  f.close()\n",
    "\n",
    "with open(\"data/data/pubtator_files/CDR_TrainingSet.PubTator.txt\", mode=\"r\") as f:\n",
    "  train_data_full = pubtator_extractor(f.readlines())\n",
    "  f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "initial_data = random.sample(train_data_full,25)\n",
    "\n",
    "with open(\"data/data/training_data/initial_training.txt\", mode=\"w\") as f:\n",
    "  for data in initial_data:\n",
    "      f.write(data[0])\n",
    "      f.write(\"\\n\")\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/initial_training.json\", mode=\"r\") as f:\n",
    "  train_data = json_parser(json.load(f))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:27:23.370979Z",
     "start_time": "2025-03-04T09:27:21.576326Z"
    },
    "id": "t56o6LlXklX0"
   },
   "outputs": [],
   "source": [
    "train = db_creator_spans(train_data)\n",
    "train.to_disk(\"data/data/spacy_db/train_spans.spacy\")\n",
    "\n",
    "dev = db_creator_spans(dev_data)\n",
    "dev.to_disk(\"data/data/spacy_db/dev_spans.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T12:38:53.386192Z",
     "start_time": "2025-03-02T12:07:41.538793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTa3qz4EklX0",
    "outputId": "3455e20c-1eaa-49ab-9d90-6c0b23dc4d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: data/models/initial_model\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'spancat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS SPANCAT  SPANS_SC_F  SPANS_SC_P  SPANS_SC_R  SCORE \n",
      "---  ------  -------------  ------------  ----------  ----------  ----------  ------\n",
      "  0       0        2936.91       2553.37        1.57        0.80       39.10    0.02\n",
      " 50     200       54966.35      49404.17       77.02       73.06       81.44    0.77\n",
      "100     400          16.55        494.16       77.91       72.65       83.97    0.78\n",
      "150     600           3.30        192.04       77.31       76.78       77.85    0.77\n",
      "200     800           6.69        156.44       77.99       78.38       77.61    0.78\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "data/models/initial_model/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train ./data/spacy/config.cfg --output ./data/models/initial_model --paths.train ./data/data/spacy_db/train_spans.spacy --paths.dev ./data/data/spacy_db/dev_spans.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:24:58.635138Z",
     "start_time": "2025-03-02T14:23:17.170132Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTkg1AH6klX0",
    "outputId": "ee1434bf-10e3-43fb-c4c0-c7983d0f82e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[38;5;4mℹ Per-component scores will be saved to output JSON file.\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved results to data/results/result_initial_model.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy benchmark accuracy ./data/models/initial_model/model-best/ ./data/data/spacy_db/dev_spans.spacy -o data/results/result_initial_model.json -P --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15OLoUJoklX1"
   },
   "source": [
    "Let's take 25 texts from the data with the least overall confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpf6IebcklX1",
    "outputId": "bad79ce2-ce0a-4806-d6ba-c598d828ba1a"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"data/models/initial_model/model-best\")\n",
    "datas = sorted_scores(nlp, train_data_full)\n",
    "with open(\"data/data/training_data/active_learning_1_test.txt\", mode=\"w\") as f:\n",
    "    for line in datas[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:28:21.968668Z",
     "start_time": "2025-03-04T09:28:21.949423Z"
    },
    "id": "Zq2QCR6yklX1"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_1.json\", mode=\"r\") as f:\n",
    "    active_learning_data_1 = json_parser(json.load(f))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:31:01.956674Z",
     "start_time": "2025-03-04T09:31:01.618179Z"
    },
    "id": "by3eTe3eklX1"
   },
   "outputs": [],
   "source": [
    "training_data_active = train_data + active_learning_data_1\n",
    "db_creator_spans(training_data_active).to_disk(\"data/data/spacy_db/train_spans.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LqWED-RklX1",
    "outputId": "3f378c11-e798-4001-ac4e-891a5e077485"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: data/models/active_1\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: data/models/active_1\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'spancat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS SPANCAT  SPANS_SC_F  SPANS_SC_P  SPANS_SC_R  SCORE \n",
      "---  ------  -------------  ------------  ----------  ----------  ----------  ------\n",
      "  0       0        3566.40       2850.62        1.57        0.80       39.10    0.02\n",
      " 28     200       61716.21      56715.90       77.70       86.92       70.24    0.78\n",
      " 57     400          70.38        963.51       80.78       81.41       80.17    0.81\n",
      " 85     600          10.34        543.73       78.26       78.92       77.61    0.78\n",
      "114     800           3.33        420.94       80.38       80.14       80.61    0.80\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "data/models/active_1/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train ./data/spacy/config.cfg --output ./data/models/active_1 --paths.train ./data/data/spacy_db/train_spans.spacy --paths.dev ./data/data/spacy_db/dev_spans.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T14:58:55.930067Z",
     "start_time": "2025-03-02T14:57:16.116356Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YG-cxhpoklX1",
    "outputId": "3ab49579-fc00-4fac-e8a1-e7776374413e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[38;5;4mℹ Per-component scores will be saved to output JSON file.\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved results to data/results/result_active_1.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy benchmark accuracy ./data/models/active_1/model-best/ ./data/data/spacy_db/dev_spans.spacy -o data/results/result_active_1.json -P --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DOg0y5pklX1"
   },
   "source": [
    "Let's try getting the scores after the first iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UCcAWSt_klX2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"data/models/active_1/model-best\")\n",
    "datas = sorted_scores(nlp, train_data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:40:10.364827Z",
     "start_time": "2025-03-04T09:40:10.317753Z"
    },
    "id": "qbQFBaUXklX2"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_2_test.txt\", mode=\"w\") as f:\n",
    "    for line in datas[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:40:17.849460Z",
     "start_time": "2025-03-04T09:40:17.420410Z"
    },
    "id": "W8_lomKwklX2"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_2.json\", mode=\"r\") as f:\n",
    "    active_learning_data_2 = json_parser(json.load(f))\n",
    "    f.close()\n",
    "training_data_active  += active_learning_data_2\n",
    "db_creator_spans(training_data_active).to_disk(\"data/data/spacy_db/train_spans.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhV09MrhklX2",
    "outputId": "4112238b-f58b-4833-f6d1-ca0ff1803b41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: data/models/active_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: data/models/active_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'spancat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS SPANCAT  SPANS_SC_F  SPANS_SC_P  SPANS_SC_R  SCORE \n",
      "---  ------  -------------  ------------  ----------  ----------  ----------  ------\n",
      "  0       0        3518.08       2891.66        1.57        0.80       39.10    0.02\n",
      " 20     200       63995.60      59011.56       79.02       72.81       86.38    0.79\n",
      " 40     400         117.00       1329.44       81.30       81.93       80.68    0.81\n",
      " 60     600          19.45        721.61       81.61       83.93       79.42    0.82\n",
      " 80     800          19.40        481.68       82.24       81.83       82.65    0.82\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "data/models/active_2/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train ./data/spacy/config.cfg --output ./data/models/active_2 --paths.train ./data/data/spacy_db/train_spans.spacy --paths.dev ./data/data/spacy_db/dev_spans.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2MAAHRXklX2",
    "outputId": "3df93dff-69ce-47c1-8e8e-d4c7dbc67c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[38;5;4mℹ Per-component scores will be saved to output JSON file.\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved results to data/results/result_active_2.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy benchmark accuracy ./data/models/active_2/model-best/ ./data/data/spacy_db/dev_spans.spacy -o data/results/result_active_2.json -P --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_gjdhljklX2"
   },
   "source": [
    "Let's try to improve the recall score for this is cycle by targeting on reducing the false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:49:26.740189Z",
     "start_time": "2025-03-02T15:47:49.494885Z"
    },
    "id": "I3TtkV1FklX2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"data/models/active_2/model-best\")\n",
    "data = get_sorted_false_negatives(nlp, train_data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T15:49:34.521622Z",
     "start_time": "2025-03-02T15:49:34.504309Z"
    },
    "id": "LTBT7ThJklX2"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_3_test.txt\", mode=\"w\") as f:\n",
    "    for line in data[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:40:21.702204Z",
     "start_time": "2025-03-04T09:40:21.153123Z"
    },
    "id": "9q1KQISMklX3"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_3.json\", mode=\"r\") as f:\n",
    "    active_learning_data_3 = json_parser(json.load(f))\n",
    "    f.close()\n",
    "training_data_active  += active_learning_data_3\n",
    "db_creator_spans(training_data_active).to_disk(\"data/data/spacy_db/train_spans.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1IXzjXAJklX3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: data/models/active_3\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: data/models/active_3\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'spancat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS SPANCAT  SPANS_SC_F  SPANS_SC_P  SPANS_SC_R  SCORE \n",
      "---  ------  -------------  ------------  ----------  ----------  ----------  ------\n",
      "  0       0        2941.08       2588.75        1.57        0.80       39.10    0.02\n",
      " 13     200       65942.20      62243.63       79.43       77.00       82.03    0.79\n",
      " 26     400         217.57       2185.33       84.11       88.24       80.35    0.84\n",
      " 40     600          36.97        888.47       85.01       87.18       82.95    0.85\n",
      " 53     800          22.16        734.68       83.65       87.52       80.10    0.84\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "data/models/active_3/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train ./data/spacy/config.cfg --output ./data/models/active_3 --paths.train ./data/data/spacy_db/train_spans.spacy --paths.dev ./data/data/spacy_db/dev_spans.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nOwjcj2SklX3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[38;5;4mℹ Per-component scores will be saved to output JSON file.\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved results to data/results/result_active_3.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy benchmark accuracy ./data/models/active_3/model-best/ ./data/data/spacy_db/dev_spans.spacy -o data/results/result_active_3.json -P --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UJMNBUf0klX3"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"data/models/active_3/model-best\")\n",
    "data = get_sorted_false_negatives(nlp, train_data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JoGJRapfklX3"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_4_test.txt\", mode=\"w\") as f:\n",
    "    for line in data[:25]:\n",
    "        f.write(line[0])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:40:28.420370Z",
     "start_time": "2025-03-04T09:40:27.673276Z"
    },
    "id": "CC6zLy07klX3"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/training_data/active_learning_4.json\", mode=\"r\") as f:\n",
    "    active_learning_data_4 = json_parser(json.load(f))\n",
    "    f.close()\n",
    "training_data_active  += active_learning_data_4\n",
    "db_creator_spans(training_data_active).to_disk(\"data/data/spacy_db/train_spans.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMKPLB0tklX3"
   },
   "source": [
    "Let's do a final training with the NER component instead of spancat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:44:54.621617Z",
     "start_time": "2025-03-04T09:44:53.116984Z"
    },
    "id": "JEWFSsClklX3"
   },
   "outputs": [],
   "source": [
    "db_creator(training_data_active).to_disk(\"data/data/spacy_db/train.spacy\")\n",
    "db_creator(dev_data).to_disk(\"data/data/spacy_db/dev.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "NCIeeEGCklX3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: data/models/final\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         330.66    712.49    7.48    4.05   48.30    0.07\n",
      " 11     200       47790.93  70766.52   82.52   86.01   79.30    0.83\n",
      " 23     400        1562.14   2875.16   82.50   81.36   83.67    0.82\n",
      " 35     600         509.21    877.58   84.12   86.43   81.92    0.84\n",
      " 47     800         294.72    498.32   84.22   84.68   83.75    0.84\n",
      " 58    1000         199.98    309.86   84.26   85.64   82.93    0.84\n",
      " 70    1200         147.23    253.51   81.75   78.15   85.69    0.82\n",
      " 82    1400         156.12    260.42   84.74   83.84   85.66    0.85\n",
      " 94    1600         141.45    235.78   84.58   83.72   85.47    0.85\n",
      "105    1800         152.75    246.39   84.44   83.34   85.58    0.84\n",
      "117    2000         131.08    213.38   84.64   84.40   84.88    0.85\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "data/models/final/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train ./data/spacy/config_final.cfg --output ./data/models/final --paths.train ./data/data/spacy_db/train.spacy --paths.dev ./data/data/spacy_db/dev.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WTiV0k22klX3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[38;5;4mℹ Per-component scores will be saved to output JSON file.\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved results to data/results/result_final.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy benchmark accuracy ./data/models/final/model-best/ ./data/data/spacy_db/dev.spacy -o data/results/result_final.json -P --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYoEDg_2klX3"
   },
   "source": [
    "Let's Evaluate our model against the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:46:29.140470Z",
     "start_time": "2025-03-04T09:46:29.102598Z"
    },
    "id": "MLLKcy70klX3"
   },
   "outputs": [],
   "source": [
    "with open(\"data/data/pubtator_files/CDR_TestSet.PubTator.txt\", mode=\"r\") as f:\n",
    "  test_data = pubtator_extractor(f.readlines())\n",
    "  f.close()\n",
    "db_creator(test_data).to_disk(\"data/data/spacy_db/test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wqpfCya_klX4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[38;5;4mℹ Per-component scores will be saved to output JSON file.\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved results to data/results/result_test.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy benchmark accuracy ./data/models/final/model-best/ ./data/data/spacy_db/test.spacy -o data/results/result_test.json -P --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
